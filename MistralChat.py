# app.py
import streamlit as st
from mistralai.client import MistralClient
from mistralai.models.chat_completion import ChatMessage
import logging
import datetime
from streamlit_feedback import streamlit_feedback # Importez le composant

# Importer nos modules locaux
from utils.config import APP_TITLE, COMMUNE_NAME, MISTRAL_API_KEY
from utils.vector_store import VectorStoreManager
from utils.database import log_interaction, update_feedback # Importez update_feedback
from utils.query_classifier import QueryClassifier

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Configuration de la page Streamlit ---
st.set_page_config(
    page_title=APP_TITLE,
    page_icon="üìö",
    layout="wide"
)

# --- Initialisation (avec mise en cache Streamlit) ---

# Met en cache le VectorStoreManager pour √©viter de recharger l'index √† chaque interaction
@st.cache_resource
def get_vector_store():
    logging.info("Chargement du VectorStoreManager...")
    return VectorStoreManager()

# Met en cache le client Mistral
@st.cache_resource
def get_mistral_client():
    if not MISTRAL_API_KEY:
        st.error("Erreur: La cl√© API Mistral (MISTRAL_API_KEY) n'est pas configur√©e.")
        st.stop()
    logging.info("Initialisation du client Mistral...")
    return MistralClient(api_key=MISTRAL_API_KEY)

# Met en cache le classificateur de requ√™tes
@st.cache_resource
def get_query_classifier():
    logging.info("Initialisation du classificateur de requ√™tes...")
    return QueryClassifier()

# Charge le Vector Store, le client Mistral et le classificateur de requ√™tes
vector_store = get_vector_store()
client = get_mistral_client()
query_classifier = get_query_classifier()

# Initialise l'historique du chat dans l'√©tat de la session s'il n'existe pas
if "messages" not in st.session_state:
    st.session_state.messages = []
# Initialise l'ID de la derni√®re interaction pour le feedback
if "last_interaction_id" not in st.session_state:
    st.session_state.last_interaction_id = None

# --- Interface Utilisateur ---

# Barre lat√©rale (sidebar)
with st.sidebar:
    st.title(f"üìö {COMMUNE_NAME}")
    st.caption(f"Assistant virtuel municipal")

    # Bouton pour lancer une nouvelle conversation
    if st.button("üîÑ Nouvelle conversation", use_container_width=True):
        # R√©initialiser l'historique des messages
        st.session_state.messages = []
        st.session_state.last_interaction_id = None
        st.rerun()  # Recharger l'application pour afficher la nouvelle conversation

    st.divider()

    # Param√®tres de l'application
    st.subheader("‚öôÔ∏è Param√®tres")

    # S√©lecteur de mod√®le Mistral
    model_options = {
        "mistral-small-latest": "Mistral Small (rapide)",
        "mistral-large-latest": "Mistral Large (pr√©cis)"
    }
    selected_model = st.selectbox(
        "Mod√®le LLM",
        options=list(model_options.keys()),
        format_func=lambda x: model_options[x],
        index=0  # Small par d√©faut
    )

    # Slider pour le nombre de documents
    num_docs = st.slider(
        "Nombre de documents √† r√©cup√©rer",
        min_value=1,
        max_value=20,
        value=5,  # 5 par d√©faut
        step=1
    )

    # Slider pour le score minimum (en pourcentage)
    min_score_percent = st.slider(
        "Score minimum (filtrer les r√©sultats faibles)",
        min_value=0,
        max_value=100,
        value=75,  # 75% par d√©faut
        step=5,
        format="%d%%"
    )
    # Convertir le pourcentage en valeur d√©cimale (0-1)
    min_score = min_score_percent / 100.0

    st.divider()

    # Informations sur l'application
    st.subheader("üìù Informations")
    st.markdown(f"**Mod√®le s√©lectionn√©**: {model_options[selected_model]}")
    st.markdown(f"**Documents index√©s**: {vector_store.index.ntotal if vector_store.index else 0}")

    # Informations sur la conversation actuelle
    if st.session_state.messages:
        st.info(f"{len(st.session_state.messages) // 2} √©changes dans cette conversation")

        # Bouton pour t√©l√©charger la conversation
        # Pr√©parer le contenu de la conversation au format texte
        conversation_text = "\n\n".join([
            f"{'Utilisateur' if msg['role'] == 'user' else 'Assistant'}: {msg['content']}"
            for msg in st.session_state.messages
        ])

        # Ajouter un en-t√™te avec la date et le titre
        header = f"Conversation avec l'assistant virtuel de {COMMUNE_NAME}\n"
        header += f"Date: {datetime.datetime.now().strftime('%d/%m/%Y %H:%M')}\n\n"
        conversation_text = header + conversation_text

        # Bouton de t√©l√©chargement
        st.download_button(
            label="üíæ T√©l√©charger la conversation",
            data=conversation_text,
            file_name=f"conversation_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.txt",
            mime="text/plain",
            use_container_width=True
        )

# Titre principal
st.title(f"üìö {APP_TITLE}")
st.caption(f"Posez vos questions sur {COMMUNE_NAME}")

# Affichage de l'historique du chat
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        # Afficher les sources si elles existent pour les messages de l'assistant
        if message["role"] == "assistant" and "sources" in message and message["sources"]:
            with st.expander("Sources utilis√©es"):
                for i, source in enumerate(message["sources"]):
                    # Acc√®s s√©curis√© aux m√©tadonn√©es
                    meta = source.get("metadata", {})
                    st.markdown(f"**Source {i+1}:** `{meta.get('source', 'N/A')}`")
                    st.markdown(f"*Score de similarit√©:* {source.get('score', 0.0):.2f}%")
                    if 'raw_score' in source:
                        st.markdown(f"*Score brut:* {source.get('raw_score', 0.0):.4f}")
                    st.markdown(f"*Cat√©gorie:* `{meta.get('category', 'N/A')}`")
                    st.text_area(f"Extrait {i+1}", value=source.get("text", "")[:500]+"...", height=100, disabled=True, key=f"src_{message['timestamp']}_{i}") # Cl√© unique pour √©viter les conflits


# Zone de saisie utilisateur en bas
if prompt := st.chat_input("Posez votre question ici..."):
    # Ajouter le message utilisateur √† l'historique et l'afficher
    st.session_state.messages.append({"role": "user", "content": prompt, "timestamp": datetime.datetime.now().isoformat()})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Afficher un message d'attente
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        message_placeholder.markdown("üß† Recherche d'informations et g√©n√©ration de la r√©ponse...")

        # --- Logique de traitement de la requ√™te ---
        try:
            # 1. Classifier la requ√™te pour d√©terminer si elle n√©cessite RAG
            needs_rag, confidence, reason = query_classifier.needs_rag(prompt)

            # Afficher le r√©sultat de la classification
            mode_str = "RAG" if needs_rag else "DIRECT"
            logging.info(f"Classification de la requ√™te: {mode_str} (confiance: {confidence:.2f}) - Raison: {reason}")

            # Afficher un message indiquant le mode utilis√©
            mode_info = st.empty()
            if needs_rag:
                mode_info.info(f"Mode RAG: Recherche d'informations sp√©cifiques dans la base de connaissances (confiance: {confidence:.2f})")
                # 2. Recherche dans le Vector Store si n√©cessaire
                logging.info(f"Recherche de documents pour: '{prompt}' (max: {num_docs}, score min: {min_score})")
                retrieved_docs = vector_store.search(prompt, k=num_docs, min_score=min_score)
            else:
                mode_info.info(f"Mode Direct: R√©ponse bas√©e sur les connaissances g√©n√©rales du mod√®le (confiance: {confidence:.2f})")
                # Pas de recherche dans le Vector Store
                retrieved_docs = []

            # 2. Pr√©parer les donn√©es en fonction du mode
            if needs_rag and retrieved_docs:
                # Mode RAG avec documents trouv√©s
                logging.info(f"{len(retrieved_docs)} documents r√©cup√©r√©s.")
                # Pr√©parer le contexte pour le LLM
                context_str = "\n\n---\n\n".join([
                    f"Source: {doc['metadata'].get('source', 'Inconnue')} (Score: {doc['score']:.4f})\nContenu: {doc['text']}"
                    for doc in retrieved_docs
                ])
                sources_for_log = [ # Version simplifi√©e pour le log et l'affichage
                    {"text": doc["text"], "metadata": doc["metadata"], "score": doc["score"]}
                    for doc in retrieved_docs
                ]

                # Prompt syst√®me pour le mode RAG
                system_prompt = f"""Vous √™tes un assistant virtuel pour {COMMUNE_NAME}.
R√©pondez √† la question de l'utilisateur en vous basant UNIQUEMENT sur le contexte fourni ci-dessous.
Si l'information n'est pas dans le contexte, dites que vous ne savez pas ou que l'information n'est pas disponible dans les documents fournis.
Soyez concis et pr√©cis. Citez vos sources si possible (par exemple, en mentionnant le nom du fichier ou la cat√©gorie trouv√©e dans les m√©tadonn√©es).

Contexte fourni:
---
{context_str}
---
"""
            elif needs_rag and not retrieved_docs:
                # Mode RAG mais aucun document trouv√©
                logging.warning("Aucun document pertinent trouv√©.")
                context_str = "Aucune information pertinente trouv√©e dans les documents."
                sources_for_log = []

                # Prompt syst√®me pour le mode RAG sans r√©sultats
                system_prompt = f"""Vous √™tes un assistant virtuel pour {COMMUNE_NAME}.
L'utilisateur a pos√© une question qui semble concerner des informations sp√©cifiques √† la commune, mais aucune information pertinente n'a √©t√© trouv√©e dans notre base de connaissances.
Indiquez poliment que vous n'avez pas cette information sp√©cifique et sugg√©rez √† l'utilisateur de reformuler sa question ou de contacter directement la mairie.
N'inventez pas d'informations sur {COMMUNE_NAME}.
"""
            else:
                # Mode Direct (sans RAG)
                context_str = "Mode direct: r√©ponse bas√©e sur les connaissances g√©n√©rales du mod√®le."
                sources_for_log = []

                # Prompt syst√®me pour le mode Direct
                system_prompt = f"""Vous √™tes un assistant virtuel pour {COMMUNE_NAME}.
R√©pondez √† la question de l'utilisateur en utilisant vos connaissances g√©n√©rales.
Soyez concis, pr√©cis et utile.
Si la question concerne des informations sp√©cifiques √† {COMMUNE_NAME} que vous ne connaissez pas, indiquez clairement que vous n'avez pas cette information sp√©cifique.
N'inventez pas d'informations sur {COMMUNE_NAME}.
"""
            user_message = ChatMessage(role="user", content=prompt)
            system_message = ChatMessage(role="system", content=system_prompt)
            messages_for_api = [system_message, user_message]

            # 3. Appel √† l'API Mistral Chat
            logging.info(f"Appel de l'API Mistral Chat avec le mod√®le {selected_model}...")
            chat_response = client.chat(
                model=selected_model,
                messages=messages_for_api
            )
            response_text = chat_response.choices[0].message.content
            logging.info("R√©ponse g√©n√©r√©e par Mistral.")

            # 4. Afficher la r√©ponse et les sources
            message_placeholder.markdown(response_text)

            # Afficher les sources si disponibles (mode RAG avec r√©sultats)
            if sources_for_log:
                with st.expander("Sources utilis√©es"):
                    for i, source in enumerate(sources_for_log):
                        meta = source.get("metadata", {})
                        st.markdown(f"**Source {i+1}:** `{meta.get('source', 'N/A')}`")
                        st.markdown(f"*Score de similarit√©:* {source.get('score', 0.0):.2f}%")
                        if 'raw_score' in source:
                            st.markdown(f"*Score brut:* {source.get('raw_score', 0.0):.4f}")
                        st.markdown(f"*Cat√©gorie:* `{meta.get('category', 'N/A')}`")
                        st.text_area(f"Extrait {i+1}", value=source.get("text", "")[:500]+"...", height=100, disabled=True, key=f"src_new_{i}") # Cl√© unique
            elif needs_rag:
                # Mode RAG sans r√©sultats
                st.info("Aucune source pertinente n'a √©t√© trouv√©e dans la base de connaissances pour cette question.")
            else:
                # Mode Direct
                st.info("R√©ponse g√©n√©r√©e en mode direct, sans consultation de la base de connaissances.")

            # 5. Enregistrer l'interaction dans la base de donn√©es (sans feedback initial)
            # Ajouter des m√©tadonn√©es sur le mode utilis√©
            metadata = {
                "mode": "RAG" if needs_rag else "DIRECT",
                "confidence": confidence,
                "reason": reason
            }

            interaction_id = log_interaction(
                query=prompt,
                response=response_text,
                sources=sources_for_log, # Stocke la liste de dicts
                metadata=metadata # Ajouter les m√©tadonn√©es sur le mode
            )
            st.session_state.last_interaction_id = interaction_id # Garde l'ID pour le feedback
            logging.info(f"Interaction enregistr√©e avec ID: {interaction_id}")


            # Ajouter la r√©ponse de l'assistant √† l'historique pour affichage permanent
            st.session_state.messages.append({
                "role": "assistant",
                "content": response_text,
                "sources": sources_for_log, # Garder les sources pour r√©affichage
                "timestamp": datetime.datetime.now().isoformat(),
                 "interaction_id": interaction_id # Lier le message √† l'ID BDD
            })


        except Exception as e:
            # V√©rifier si c'est une erreur API Mistral
            if hasattr(e, 'status_code') and hasattr(e, 'message'):
                logging.error(f"Erreur API Mistral: {e}")
                message_placeholder.error(f"Une erreur s'est produite lors de la communication avec l'API Mistral: {e}")
            else:
                logging.error(f"Erreur inattendue: {e}", exc_info=True)
                message_placeholder.error(f"Une erreur s'est produite: {e}")

            st.session_state.messages.append({"role": "assistant", "content": f"Erreur: {e}", "sources": [], "timestamp": datetime.datetime.now().isoformat(), "interaction_id": None})
            st.session_state.last_interaction_id = None # Pas d'ID si erreur avant log

# --- Section Feedback ---
# Placer le feedback apr√®s la boucle d'affichage et la zone de chat input
# On cible la *derni√®re* r√©ponse de l'assistant pour le feedback
last_assistant_message = next((m for m in reversed(st.session_state.messages) if m["role"] == "assistant"), None)

# V√©rifie si la derni√®re r√©ponse a un ID d'interaction associ√©
current_interaction_id = last_assistant_message.get("interaction_id") if last_assistant_message else None

if current_interaction_id:
    # Utilisation de streamlit-feedback
    feedback = streamlit_feedback(
        feedback_type="thumbs", # "thumbs" ou "faces"
        optional_text_label="[Optionnel] Commentaires :",
        key=f"feedback_{current_interaction_id}", # Cl√© unique li√©e √† l'interaction
        align="flex-start",  # Aligner √† gauche
        on_submit=lambda x: logging.info(f"Feedback soumis: {x}")  # Log pour d√©bogage
    )

    # Traitement du feedback s'il est donn√©
    if feedback:
        # Convertir le feedback en valeur num√©rique et texte
        feedback_score = feedback.get('score')

        # V√©rifier si le score est valide
        # Le composant streamlit_feedback peut renvoyer des emojis au lieu de "thumbs_up"/"thumbs_down"
        if feedback_score == "üëç" or feedback_score == "thumbs_up":
            feedback_score = "positive"
        elif feedback_score == "üëé" or feedback_score == "thumbs_down":
            feedback_score = "negative"
        else:
            logging.warning(f"Score de feedback invalide: {feedback_score}")
            feedback_score = None

        # 1 pour positif, 0 pour n√©gatif
        feedback_value = 1 if feedback_score == "positive" else 0 if feedback_score == "negative" else None

        # Texte pour la base de donn√©es ("positif" ou "n√©gatif")
        feedback_text = "positif" if feedback_score == "positive" else "n√©gatif" if feedback_score == "negative" else "N/A"

        # Emoji pour l'affichage dans l'interface
        feedback_emoji = "üëç" if feedback_score == "positive" else "üëé" if feedback_score == "negative" else "N/A"
        comment = feedback.get('text', None)

        # Mettre √† jour l'interaction dans la base de donn√©es
        success = update_feedback(current_interaction_id, feedback_text, comment, feedback_value)
        if success:
            st.toast(f"Merci pour votre retour ({feedback_emoji}) !", icon="‚úÖ")
            # Optionnel: D√©sactiver les boutons apr√®s le premier clic pour √©viter les soumissions multiples
            # Ceci est plus complexe √† g√©rer avec la nature stateless de Streamlit sans callbacks avanc√©s.
            # Pour la simplicit√©, on se contente de l'enregistrer. L'utilisateur peut re-cliquer mais seule la derni√®re valeur compte.
        else:
            st.toast("Erreur lors de l'enregistrement de votre retour.", icon="‚ùå")

        # Optionnel : Effacer le feedback de l'√©tat pour √©viter re-soumission au re-run
        # st.session_state[f"feedback_{current_interaction_id}"] = None # Peut causer des pbs si mal g√©r√©

else:
    st.write("Posez une question pour pouvoir donner votre avis sur la r√©ponse.")