{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Démonstration des techniques de découpage de texte avec Python\n","---\n","\n","Dans ce notebook, nous explorons plusieurs techniques de découpage de texte, utiles dans le cadre de tâches de NLP (traitement du langage naturel), de résumé, ou d'indexation pour des agents conversationnels.\n","\n","Nous allons couvrir :\n","\n","- Le découpage récursif avec chevauchement (LangChain)\n","\n","- Le découpage basé sur les balises (HTML/Markdown)\n","\n","- Le découpage sémantique avec un modèle NLP (spaCy)"],"metadata":{"id":"ePITRl6Dhegp"}},{"cell_type":"code","source":["# Installation des dépendances\n","!pip install langchain --quiet\n","!pip install spacy --quiet\n","!python -m spacy download fr_core_news_sm"],"metadata":{"id":"16c-DiaqmMHS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texte = \"\"\"\n","# L'Importance du Découpage de Texte (Chunking)\n","\n","Le découpage de texte, ou \"chunking\", est une technique fondamentale dans le traitement automatique du langage naturel (NLP) et la gestion de l'information. Elle consiste à diviser un long texte en morceaux plus petits et plus gérables, appelés \"chunks\". Cette étape est souvent cruciale avant d'appliquer d'autres traitements, comme l'indexation pour les moteurs de recherche, l'analyse de sentiments, ou l'alimentation de modèles de langage étendus (LLMs) dans des systèmes comme Retrieval-Augmented Generation (RAG).\n","\n","## Pourquoi découper le texte ?\n","\n","Les raisons sont multiples. Premièrement, de nombreux modèles d'IA ont une limite sur la taille du contexte qu'ils peuvent traiter en une seule fois (la \"context window\"). Découper le texte permet de respecter ces limites. Deuxièmement, pour des tâches comme la recherche d'information, analyser des segments plus courts et ciblés peut donner de meilleurs résultats que d'analyser un document entier d'un seul bloc. Cela permet d'identifier plus précisément les passages pertinents. Enfin, cela facilite la parallélisation des traitements sur de grands volumes de données textuelles.\n","\n","## Différentes Approches de Découpage\n","\n","Il n'existe pas une unique \"bonne\" façon de découper un texte. Le choix de la méthode dépend fortement du type de contenu et de l'objectif final. Voici quelques approches courantes que nous pouvons comparer :\n","\n","* **Le découpage récursif par caractères :** Cette méthode vise à créer des chunks d'une taille approximativement fixe (en nombre de caractères), avec un certain chevauchement pour ne pas perdre le contexte aux frontières. Elle essaie de couper sur des séparateurs logiques (paragraphes `\\n\\n`, phrases `. `, mots ` `) avant de couper brutalement si nécessaire pour respecter la taille. C'est **simple mais potentiellement abrupt**.\n","\n","* **Le découpage basé sur la structure Markdown :** Idéal pour les contenus rédigés en Markdown. Cette technique utilise la structure inhérente du document (titres `#`, `##`, etc., paragraphes séparés par des lignes vides, listes `*` ou `-`, blocs de code ``` ```, citations `>`) pour définir les chunks. Un parser Markdown peut identifier ces éléments, et chacun peut devenir un chunk. Cela **respecte la logique organisationnelle** du document voulue par l'auteur.\n","\n","* **Le découpage sémantique :** Cette approche plus avancée utilise des modèles NLP pour identifier les frontières naturelles du discours, comme la fin des phrases ou des groupes de phrases traitant d'un même sous-sujet. L'objectif est de créer des chunks qui sont *cohérents sémantiquement*, même si leur taille varie. Cela nécessite des outils plus complexes (comme spaCy ou des modèles d'embedding) mais peut préserver le sens de manière plus efficace.\n","\n","## Exemple Concret et Comparaison\n","\n","Imaginons appliquer ces trois techniques à ce document Markdown même.\n","Le découpage récursif pourrait couper le milieu d'un paragraphe s'il dépasse la taille cible, en essayant d'abord de couper entre les paragraphes (sur `\\n\\n`) ou les phrases.\n","Le découpage basé sur la structure Markdown créerait probablement un chunk distinct pour chaque titre (`#`, `##`), chaque paragraphe (bloc de texte séparé par une ligne vide), et chaque élément de la liste (`*`).\n","Le découpage sémantique essaierait de regrouper les phrases qui parlent d'un même concept (par exemple, l'explication du découpage récursif) pour former un chunk, même si cela couvre plusieurs lignes ou éléments Markdown.\n","\n","## Conclusion\n","\n","En conclusion, le choix de la technique de chunking est une étape de conception importante. Il faut considérer la nature du texte (structuré en Markdown ou non, long ou court) et l'usage qui sera fait des chunks (recherche, résumé, alimentation d'un LLM). Une analyse comparative sur des exemples réels comme celui-ci est souvent nécessaire pour choisir la méthode la plus adaptée à vos besoins spécifiques.\n","\"\"\""],"metadata":{"id":"4MpFMayAnREV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","# Initialisation du splitter\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000,      # Taille de chaque segment\n","    chunk_overlap=200     # Chevauchement entre les segments\n",")\n","\n","# Exemple d'utilisation\n","\n","segments = text_splitter.split_text(texte)\n","\n","for i, segment in enumerate(segments):\n","    print(f\"Segment {i+1}:\\n{segment}\\n\")"],"metadata":{"id":"niKrW8nkgpN3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importation nécessaire depuis LangChain\n","from langchain.text_splitter import MarkdownHeaderTextSplitter\n","\n","# Définir les en-têtes sur lesquels on veut découper le texte.\n","# Chaque tuple représente un niveau de titre ('#' pour h1, '##' pour h2, etc.)\n","# et le nom associé à ce niveau dans les métadonnées du chunk.\n","headers_to_split_on = [\n","    (\"#\", \"Header 1\"),\n","    (\"##\", \"Header 2\"),\n","]\n","\n","# Initialiser le splitter\n","markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n","\n","# Découper le texte\n","md_header_splits = markdown_splitter.split_text(texte)\n","\n","# Afficher les chunks résultants (chaque chunk est un Document LangChain\n","# avec du contenu 'page_content' et des 'metadata')\n","for i, split in enumerate(md_header_splits):\n","    print(f\"--- Chunk {i+1} ---\")\n","    print(f\"Métadonnées: {split.metadata}\")\n","    print(f\"Contenu:\\n{split.page_content}\")\n","    print(\"-\" * 20)\n"],"metadata":{"id":"gU8z_UuagpRK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","\n","# Charger le modèle français\n","nlp = spacy.load(\"fr_core_news_sm\")\n","\n","def semantic_chunking(text, max_chunk_size=150):\n","    doc = nlp(text)\n","    segments = []\n","    current_segment = []\n","\n","    for sent in doc.sents:\n","        if len(\" \".join(current_segment)) + len(sent.text) <= max_chunk_size:\n","            current_segment.append(sent.text)\n","        else:\n","            segments.append(\" \".join(current_segment))\n","            current_segment = [sent.text]\n","    if current_segment:\n","        segments.append(\" \".join(current_segment))\n","    return segments\n","\n","segments = semantic_chunking(texte)\n","\n","for i, segment in enumerate(segments):\n","    print(f\"Segment {i+1}:\\n{segment}\\n\")"],"metadata":{"id":"MXQx_0CdgpUZ"},"execution_count":null,"outputs":[]}]}